{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python -m virtualenv -p python3.12 .venv\n",
    "# !python -m pip install torch torchvision torchaudio\n",
    "# !pip install tensorboard\n",
    "# !pip install onnx\n",
    "# !pip install onnxscript\n",
    "# !python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "# !pip install matplotlib\n",
    "# !pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer Un dataset pr√©fait (MNIST)\n",
    "\n",
    "training_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])\n"
     ]
    }
   ],
   "source": [
    "print(training_dataset)\n",
    "print(training_dataset.data.shape)\n",
    "print(training_dataset.targets.shape)\n",
    "print(training_dataset.targets.unique())\n",
    "print(training_dataset.targets.bincount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_dataset), size=(1,)).item()\n",
    "    img, label = training_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X : torch.Size([64, 1, 28, 28])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"shape X : {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fcs = nn.ModuleList(\n",
    "            [nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)]\n",
    "        )\n",
    "        self.act = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) #1 * 28 * 28 -> 1 * 784 \n",
    "        for layer in self.fcs:\n",
    "            x  = layer(x)\n",
    "            x = self.act(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10 #Nombre de classe\n",
    "model = MLP([input_dim, 128, 128, output_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fcs): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
       "  )\n",
       "  (act): ReLU()\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # Expect raw logits (!= probabilities)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "        #Pr√©diction\n",
    "        ypred = model(X)\n",
    "        loss = loss_fn(ypred, y) #Calcul de l'erreur\n",
    "\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step() #W = W - lr * grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size * len(X)\n",
    "            print(f'loss {loss:>7f} [{current:>5d}/{len(dataloader.dataset)}]')\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            ypred = model(X)\n",
    "            test_loss += loss_fn(ypred, y).item()\n",
    "            correct += (ypred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "    test_loss/= len(dataloader)\n",
    "    correct/=len(dataloader.dataset)\n",
    "    print(f\"Test loss: {test_loss:>8f} | test accuracy {(correct * 100):>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1} ------------------------\")\n",
    "#     train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "#     test_loop(test_dataloader, model, loss_fn)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataset = datasets.MNIST(\n",
    "#     root=\"data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=T.ToTensor()\n",
    "# )\n",
    "\n",
    "# test_dataset = datasets.MNIST(\n",
    "#     root=\"data\",\n",
    "#     train=False,\n",
    "#     download=True,\n",
    "#     transform=T.ToTensor()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 2.989267386510619e-06\n",
      "Variance : 0.30183959007263184\n"
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import ConcatDataset\n",
    "\n",
    "# mean, std = 0,0\n",
    "# combined_datasets = ConcatDataset([training_dataset, test_dataset])\n",
    "\n",
    "# for image, label in combined_datasets:\n",
    "#     image = image.view(28*28) \n",
    "#     mean = image.mean().sum()\n",
    "#     std += image.std().sum()\n",
    "\n",
    "# mean/= len(combined_datasets)\n",
    "# std/=len(combined_datasets)\n",
    "\n",
    "# print(f'Mean : {mean}')\n",
    "# print(f'Variance : {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_v2 = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=T.Compose([T.ToTensor(),T.Normalize((mean),(std))])\n",
    ")\n",
    "\n",
    "test_dataset_v2 = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=T.Compose([T.ToTensor(),T.Normalize((mean),(std))])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean : 9.903354111884255e-06\n",
      "Variance : 0.9999989867210388\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "mean, std = 0,0\n",
    "combined_datasets = ConcatDataset([training_dataset_v2, test_dataset_v2])\n",
    "\n",
    "for image, label in combined_datasets:\n",
    "    image = image.view(28*28) \n",
    "    mean = image.mean().sum()\n",
    "    std += image.std().sum()\n",
    "\n",
    "mean/= len(combined_datasets)\n",
    "std/=len(combined_datasets)\n",
    "\n",
    "print(f'Mean : {mean}')\n",
    "print(f'Variance : {std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape X : torch.Size([64, 1, 28, 28])\n",
      "y shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader_v2 = DataLoader(training_dataset_v2, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader_v2 = DataLoader(test_dataset_v2, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"shape X : {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ------------------------\n",
      "loss 2.301116 [    0/60000]\n",
      "loss 1.317920 [409600/60000]\n",
      "loss 1.116725 [819200/60000]\n",
      "loss 1.132330 [1228800/60000]\n",
      "loss 1.377505 [1638400/60000]\n",
      "loss 1.021364 [2048000/60000]\n",
      "loss 1.055431 [2457600/60000]\n",
      "loss 0.919061 [2867200/60000]\n",
      "loss 1.229847 [3276800/60000]\n",
      "loss 1.220198 [3686400/60000]\n",
      "Test loss: 1.067155 | test accuracy 56.540000\n",
      "Epoch 2 ------------------------\n",
      "loss 0.871453 [    0/60000]\n",
      "loss 0.799095 [409600/60000]\n",
      "loss 1.060146 [819200/60000]\n",
      "loss 0.941705 [1228800/60000]\n",
      "loss 1.025890 [1638400/60000]\n",
      "loss 1.037637 [2048000/60000]\n",
      "loss 1.237923 [2457600/60000]\n",
      "loss 1.082995 [2867200/60000]\n",
      "loss 1.081439 [3276800/60000]\n",
      "loss 1.032664 [3686400/60000]\n",
      "Test loss: 1.049539 | test accuracy 56.730000\n",
      "Epoch 3 ------------------------\n",
      "loss 0.969851 [    0/60000]\n",
      "loss 1.132389 [409600/60000]\n",
      "loss 1.160665 [819200/60000]\n",
      "loss 0.918874 [1228800/60000]\n",
      "loss 0.778607 [1638400/60000]\n",
      "loss 1.151304 [2048000/60000]\n",
      "loss 0.937052 [2457600/60000]\n",
      "loss 0.909917 [2867200/60000]\n",
      "loss 1.122742 [3276800/60000]\n",
      "loss 1.293760 [3686400/60000]\n",
      "Test loss: 1.033798 | test accuracy 57.010000\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} ------------------------\")\n",
    "    train_loop(train_dataloader_v2, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader_v2, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc√©l√©rateur: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Acc√©l√©rateur: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim = 28*28\n",
    "# output_dim = 10 #Nombre de classe\n",
    "# model = MLP([input_dim, 128, 128, output_dim]).to(device)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss().to(device) # Expect raw logits (!= probabilities)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('run/mnist_experiment_11')\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    return epoch_loss / len(dataloader), correct / len(dataloader.dataset) * 100\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    return test_loss / len(dataloader), correct / len(dataloader.dataset) * 100\n",
    "\n",
    "# Training loop with added graphing\n",
    "def train_and_test(dataloader_train, dataloader_test, model, loss_fn, optimizer, epochs, batch_size):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # Train the model\n",
    "        train_loss, accuracy = train_loop(dataloader_train, model, loss_fn, optimizer, batch_size)\n",
    "        writer.add_scalars('Loss', {'train': train_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'train': accuracy}, epoch)\n",
    "\n",
    "        # Test the model\n",
    "        test_loss, accuracy = test_loop(dataloader_test, model, loss_fn)\n",
    "        writer.add_scalars('Loss', {'test': test_loss}, epoch)\n",
    "        writer.add_scalars('Accuracy', {'test': accuracy}, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} complete\\n\")\n",
    "\n",
    "# train_and_test(train_dataloader_v2, test_dataloader_v2, model, loss_fn, optimizer, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=runs\n",
    "# #voir les logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1), #1 * 28 *28 -> 32 * 28 * 28\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1), #32 * 28 *28 -> 64 * 28 * 28\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(0.2),\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2) #64 * 28 *28 -> 64 * 14 * 14\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 14 * 14, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv_stack(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn  = CNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device) # Expect raw logits (!= probabilities)\n",
    "optimizer = torch.optim.AdamW(cnn.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Epoch 1/1 complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "train_and_test(train_dataloader_v2, test_dataloader_v2, cnn, loss_fn, optimizer, epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 17968), started 1:16:40 ago. (Use '!kill 17968' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b68d34f3d7ee08d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b68d34f3d7ee08d\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n",
    "#voir les logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.to('cpu')\n",
    "torch_input = torch.randn(1,1,28,28) # n, c , L, T\n",
    "onn_program = torch.onnx.export(\n",
    "    cnn,\n",
    "    torch_input,\n",
    "    \"model.onnx\",\n",
    "    verbose=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=11, # /!\\\n",
    "    export_params=True,\n",
    "    do_constant_folding=True,\n",
    "    dynamic_axes={\n",
    "        \"input\":{0:\"batch_size\"},\n",
    "        \"output\":{0:\"batch_size\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1\n",
      "Sample input: [tensor([[[[-1.6477e+00,  1.6330e+00, -4.7823e-03,  6.3573e-01, -9.9606e-01,\n",
      "            1.2458e+00, -4.0261e-01,  5.2798e-01,  2.1038e+00, -2.3587e-01,\n",
      "           -1.2092e+00, -1.2500e-01, -2.9541e-01,  8.2340e-01,  4.3593e-01,\n",
      "            1.2053e+00,  1.5871e+00,  4.9679e-01, -5.0570e-02,  1.1742e+00,\n",
      "           -1.1369e+00,  5.1194e-01, -6.3939e-02,  1.2386e+00,  3.3472e-01,\n",
      "           -1.0642e+00, -8.0183e-01, -6.2074e-01],\n",
      "          [ 2.5839e-01,  4.7886e-01, -3.8655e-01, -1.6217e-01, -4.0382e-01,\n",
      "            1.7971e+00,  1.2729e+00,  3.9250e-02, -1.5724e-01,  5.5973e-02,\n",
      "           -1.3233e-02, -1.7176e-01,  1.2943e+00,  1.6034e+00, -2.0226e+00,\n",
      "           -2.2046e-01, -1.4912e-01, -7.5612e-01,  9.5081e-01, -5.9914e-01,\n",
      "           -6.4623e-01,  1.6136e-01, -2.1326e+00,  8.9788e-01, -8.3814e-02,\n",
      "           -5.7917e-01, -5.8826e-01, -4.9915e-01],\n",
      "          [ 9.3704e-02,  1.0624e+00, -1.7228e+00, -4.5936e-01, -1.4044e+00,\n",
      "           -1.1697e+00,  1.9277e+00, -1.2188e+00, -4.0205e-01,  4.4904e-01,\n",
      "            9.9252e-01, -1.6949e+00,  1.9175e+00,  1.4529e+00, -6.7491e-01,\n",
      "           -6.3651e-02,  5.7307e-01, -7.6969e-01, -1.2543e-01, -1.4540e+00,\n",
      "           -4.2923e-01,  4.9449e-01,  2.2745e-02,  1.0538e+00,  3.7096e-01,\n",
      "           -1.3097e+00, -5.7007e-01,  5.5445e-01],\n",
      "          [ 1.5840e+00, -2.4658e-01,  6.1742e-01,  1.9095e-01, -1.2952e+00,\n",
      "           -4.0302e-02,  8.1386e-01,  2.6391e-01,  1.9580e+00,  1.7394e-02,\n",
      "            7.8849e-01, -5.6731e-02, -1.5277e-01, -8.8379e-01,  4.6409e-01,\n",
      "           -9.6548e-02, -5.4242e-01,  4.6611e-01,  3.8400e-01,  8.6616e-02,\n",
      "            3.7543e-01, -1.9328e+00,  4.1218e-02,  2.4002e+00,  6.0368e-01,\n",
      "            7.6847e-01,  1.2479e+00, -1.7964e+00],\n",
      "          [ 6.0837e-01,  1.6465e+00,  1.1913e+00,  6.3118e-01, -5.6324e-01,\n",
      "           -1.7171e+00, -9.0748e-01, -1.2293e+00,  3.3578e-01, -7.5851e-01,\n",
      "           -1.2579e+00,  6.6845e-01, -1.8274e+00, -5.2181e-01, -2.6559e+00,\n",
      "            1.8168e+00, -1.7174e+00, -1.2165e+00,  5.2058e-02, -1.1192e+00,\n",
      "            5.3149e-01,  4.9017e-01, -9.8100e-01, -9.7634e-01, -1.3854e+00,\n",
      "            9.0644e-01, -6.7718e-01, -6.7550e-01],\n",
      "          [-1.4031e+00,  1.0778e+00, -5.2630e-01, -3.2412e-01,  4.4746e-01,\n",
      "           -2.0069e-01, -1.5817e-01, -1.0220e+00,  2.7803e-01, -1.3820e+00,\n",
      "           -1.4004e+00, -8.6208e-01, -4.4397e-01,  2.7516e-01,  1.9555e-01,\n",
      "           -3.9257e-01, -7.9948e-01, -1.1539e+00,  4.0480e-01,  8.5885e-01,\n",
      "            1.1307e+00,  4.3576e-01, -9.1654e-01, -1.4684e+00,  3.5425e-01,\n",
      "           -1.0610e+00,  9.5507e-01,  2.8531e-01],\n",
      "          [-6.2994e-01,  5.7463e-01,  9.9912e-01,  1.2591e+00,  4.1105e-02,\n",
      "            6.6298e-01, -6.7259e-01, -7.7641e-01, -1.7199e+00,  1.1682e+00,\n",
      "           -3.9102e-02, -4.2544e-03, -7.1717e-01, -3.9685e-01, -7.7996e-01,\n",
      "           -1.0930e-02,  1.0493e+00, -1.1757e+00,  1.9548e-01,  1.5915e-01,\n",
      "            1.1121e+00,  2.6635e-01, -9.2378e-01, -1.2491e+00,  1.9608e+00,\n",
      "           -1.1066e+00,  5.3981e-01,  1.4754e+00],\n",
      "          [ 1.2448e-01,  2.2768e-01,  4.5084e-01,  8.6457e-01,  5.9250e-01,\n",
      "           -3.3087e-01,  2.4660e-01,  1.1165e+00,  9.2341e-01, -7.5862e-01,\n",
      "           -5.4308e-01, -1.3091e+00,  6.6600e-01, -1.3258e+00,  3.1525e-01,\n",
      "           -5.4429e-01, -8.9478e-01,  2.2958e+00, -3.9576e-01, -9.9423e-01,\n",
      "            2.2480e+00, -1.9661e-02, -1.8435e+00,  3.4145e-01, -6.4292e-01,\n",
      "            1.2072e+00,  4.1367e-01, -9.1449e-01],\n",
      "          [-2.2058e-01, -1.9335e+00,  3.5839e+00, -2.3300e-01,  1.2569e+00,\n",
      "            1.1082e+00, -1.2069e+00,  2.4985e+00, -4.0769e-01,  5.8065e-01,\n",
      "           -1.0170e+00, -1.1207e+00, -1.6096e+00, -6.9799e-01,  1.6266e+00,\n",
      "            1.6202e+00,  6.4989e-01,  1.3779e-02,  3.8505e-01, -2.5571e-01,\n",
      "            2.5507e-04, -1.3437e+00,  2.8596e-01,  1.0282e+00, -1.2496e+00,\n",
      "            4.0787e-01,  1.9687e+00,  6.5334e-01],\n",
      "          [ 1.1335e+00,  4.0203e-01,  7.6574e-02, -7.0833e-01, -7.1807e-01,\n",
      "            6.4728e-01, -8.7339e-01,  5.0774e-01,  5.8148e-01,  7.2924e-01,\n",
      "           -1.6188e-02, -6.2640e-01, -5.8549e-01, -5.8244e-01, -9.6554e-01,\n",
      "           -3.5901e-01, -3.7699e-01, -2.3070e-01,  4.6059e-01, -6.7814e-01,\n",
      "            3.2294e+00, -7.6900e-01,  6.9337e-01, -7.3158e-01, -9.7732e-01,\n",
      "           -8.6190e-01, -6.1981e-01, -9.5831e-01],\n",
      "          [-1.2098e+00,  1.3437e+00, -4.8851e-01,  1.1067e+00,  8.1469e-01,\n",
      "           -5.7813e-01,  7.6361e-01, -7.3254e-01, -9.5448e-02,  2.7043e-01,\n",
      "            7.0641e-01, -1.5131e+00,  1.8992e+00, -5.0947e-01,  4.4968e-01,\n",
      "            2.1874e+00,  2.7977e+00,  1.7767e+00,  5.5943e-01, -2.6169e-01,\n",
      "            3.4779e-01, -7.2231e-01, -2.4858e-01, -1.1752e-01,  5.9225e-01,\n",
      "           -5.3889e-01,  3.4776e-01,  1.8217e-02],\n",
      "          [ 1.8788e+00, -2.5357e-01, -2.4717e-01, -2.1073e+00,  2.6197e+00,\n",
      "           -9.2369e-03,  7.7913e-01, -3.3369e-01,  1.4629e-01,  1.3300e+00,\n",
      "           -2.6352e-01,  1.7345e+00, -1.4010e-01, -4.8927e-02, -7.0170e-01,\n",
      "           -1.0304e+00, -5.2435e-01,  2.5010e-01,  2.0767e-01,  6.3817e-01,\n",
      "           -1.0716e+00,  2.1313e-01,  9.2063e-01, -6.3080e-01,  2.7593e+00,\n",
      "           -4.1816e-01,  9.7637e-01, -5.5066e-01],\n",
      "          [-2.5501e-01, -2.0485e-01, -3.3022e-02, -7.0261e-01, -2.0988e-01,\n",
      "           -5.9512e-01,  2.9201e-01,  1.7041e+00,  3.5648e-01, -1.0307e-01,\n",
      "           -4.4846e-01, -1.4338e-01,  4.8454e-01, -2.6896e-01,  6.8865e-01,\n",
      "           -5.5455e-01, -6.5335e-01, -1.8175e+00,  1.6459e+00, -1.1808e+00,\n",
      "           -1.2484e+00,  1.0258e+00, -1.9095e+00,  2.8068e-01,  3.7909e-01,\n",
      "            6.5841e-01, -8.3490e-02, -1.0723e+00],\n",
      "          [-1.2978e+00, -7.4200e-01, -1.0299e+00, -6.9337e-01, -2.9103e-02,\n",
      "           -7.6141e-01,  1.7232e+00,  6.3958e-01,  2.7535e-01, -9.9836e-01,\n",
      "           -9.9749e-01,  1.5509e+00,  4.6466e-01,  4.8135e-01,  6.8795e-01,\n",
      "           -3.9605e-01,  6.9464e-01,  1.3289e-01,  2.0353e-01,  5.2303e-01,\n",
      "           -8.2623e-01, -9.6720e-01,  1.3430e-01,  8.1307e-01,  1.4063e+00,\n",
      "           -2.8173e+00,  2.0133e+00, -2.4806e-01],\n",
      "          [-1.1100e+00, -2.7508e-01,  1.0270e+00, -4.5601e-01, -1.1223e+00,\n",
      "            2.2389e-01, -8.0652e-01, -6.1253e-01, -1.3306e+00, -1.2383e+00,\n",
      "            1.6688e+00,  1.1427e+00, -1.4690e+00,  3.6438e-01,  5.4366e-01,\n",
      "           -8.9004e-01, -6.1047e-01, -1.1527e+00,  1.3025e+00, -1.5613e+00,\n",
      "           -6.7215e-01,  1.0703e-01, -8.3690e-01,  7.0563e-02, -4.1696e-01,\n",
      "            1.1452e-01,  6.1615e-01, -5.3355e-01],\n",
      "          [-2.7879e-01,  9.9130e-01, -1.0343e+00, -9.3056e-02,  1.5204e+00,\n",
      "            3.7104e-02,  1.6419e-01,  5.8547e-01, -9.2007e-01, -4.0987e-01,\n",
      "           -8.6814e-01, -1.1043e+00,  1.2525e+00, -8.4640e-02,  1.1261e+00,\n",
      "            2.2419e-01,  1.2906e-01,  9.6213e-01, -6.0263e-01,  4.3235e-01,\n",
      "            3.8814e-01,  2.3392e-01, -6.1590e-01,  5.3444e-01,  3.5256e-01,\n",
      "            3.4635e-02, -6.6222e-01, -5.3787e-01],\n",
      "          [-6.3725e-01,  9.1131e-01, -5.9137e-01, -3.9565e-01,  4.1607e-02,\n",
      "           -1.6826e+00,  7.1939e-01, -1.5053e+00,  5.0893e-01, -1.1816e+00,\n",
      "            4.8990e-01, -2.6686e-01,  1.0886e-02, -1.4000e-01, -5.9918e-01,\n",
      "           -6.4786e-01,  8.5867e-01,  1.3417e+00,  4.3977e-01,  3.0825e-01,\n",
      "            6.5691e-01,  2.3714e+00,  7.0042e-01, -2.2751e+00,  1.6566e+00,\n",
      "            7.2039e-01, -7.5497e-01, -1.1117e+00],\n",
      "          [-1.0878e+00, -6.6710e-01, -7.5950e-01,  1.2707e+00,  1.1315e+00,\n",
      "            5.8934e-01, -1.7059e+00, -6.2257e-01,  1.3041e-01, -1.1109e+00,\n",
      "           -2.7163e-02,  4.7668e-02,  8.7955e-01,  5.0788e-01,  8.1137e-02,\n",
      "            1.5530e+00, -3.5737e-01, -4.8453e-01, -2.7002e+00, -6.8499e-01,\n",
      "           -7.9848e-01, -1.2758e-01,  3.1187e-01,  1.1572e+00, -1.4354e+00,\n",
      "           -1.6977e+00, -9.2870e-01, -3.2849e-01],\n",
      "          [ 9.5359e-02, -7.8776e-01, -2.1112e+00, -7.7735e-02,  2.4993e+00,\n",
      "            8.0013e-01,  1.3899e+00, -3.5564e-01,  4.5659e-01,  5.4526e-01,\n",
      "           -7.7702e-01,  3.5184e-01, -9.9279e-01,  1.6948e+00,  2.9449e-02,\n",
      "            1.0190e+00,  3.3534e-02,  2.6210e+00,  8.2136e-01, -9.9949e-02,\n",
      "           -2.1500e+00,  1.3038e+00, -2.0611e+00,  7.0583e-02,  2.5573e-01,\n",
      "           -1.1334e-01,  9.8408e-01, -3.5877e-01],\n",
      "          [-2.5252e-01,  9.3804e-01,  3.4803e-01,  3.8518e-01, -2.3152e-01,\n",
      "            1.9136e-01, -1.7973e+00,  2.4159e-01, -1.2117e+00, -4.6317e-01,\n",
      "           -5.0408e-01, -3.6849e-01,  4.0538e-01, -5.5804e-01,  4.1633e-01,\n",
      "            1.0128e+00,  2.1726e+00, -6.9959e-01,  3.9482e-01,  2.0967e-01,\n",
      "           -1.0576e+00, -1.0774e+00, -1.9076e+00, -9.0728e-01,  5.4494e-01,\n",
      "           -1.4086e-02, -1.0827e+00,  2.4401e-01],\n",
      "          [ 5.2802e-01, -2.9952e-01,  1.5272e-01, -3.5227e-01, -9.0980e-01,\n",
      "           -3.3670e-01, -1.4025e-01,  9.9253e-01, -1.4634e-01, -5.2718e-01,\n",
      "           -5.4827e-01, -4.6335e-01,  1.1450e+00,  4.2634e-01, -5.8299e-01,\n",
      "            1.5194e+00,  1.2889e+00, -8.0552e-01,  6.1926e-01, -6.7552e-01,\n",
      "           -7.4338e-01, -5.6982e-01, -4.8787e-01, -1.0107e+00,  4.0351e-01,\n",
      "            7.0271e-01,  1.6196e+00,  9.3409e-01],\n",
      "          [ 5.4446e-02,  6.2214e-01, -1.1246e+00,  9.4241e-01,  1.3475e-01,\n",
      "            9.1765e-01, -4.3079e-01, -2.7800e-01, -2.2317e-01,  6.5217e-01,\n",
      "            1.4527e+00, -1.8561e+00, -4.5461e-01,  1.8093e-01,  5.4824e-01,\n",
      "           -3.5662e-01,  4.0153e-01,  1.6041e-01,  1.0751e+00,  9.3565e-02,\n",
      "            7.8980e-01, -1.5868e+00, -9.1873e-03, -1.4055e+00,  3.8784e-01,\n",
      "            7.6929e-01, -7.0461e-01,  3.0104e-01],\n",
      "          [ 1.0244e+00,  4.3237e-01, -1.3849e+00,  3.7398e-01, -6.9496e-02,\n",
      "            2.9889e-02, -2.5969e-01, -1.8144e+00, -6.9139e-01,  9.4608e-01,\n",
      "            5.2308e-01,  4.2338e-01,  5.6322e-01, -5.0123e-01, -8.1201e-01,\n",
      "           -9.7926e-01,  1.5792e+00, -1.7339e-02,  7.5580e-01, -1.2241e+00,\n",
      "            1.5024e-01,  1.3658e-01, -5.3408e-01,  2.0838e+00, -4.3488e-02,\n",
      "            2.7009e+00, -1.0843e+00, -5.5494e-01],\n",
      "          [ 4.0532e-01,  2.3005e-01, -2.0551e-04,  6.3948e-01, -4.4867e-01,\n",
      "           -1.5065e+00,  3.1102e-01,  9.2342e-01,  1.1180e+00, -3.9662e-01,\n",
      "            2.2944e+00, -2.2938e+00, -9.6133e-01,  1.8723e+00, -5.3928e-02,\n",
      "            7.8821e-01,  9.2281e-01,  6.6134e-02, -8.9080e-03, -1.3458e+00,\n",
      "           -2.7270e-01,  2.8633e-01, -2.0969e+00, -2.8100e-01, -7.2350e-01,\n",
      "            1.8019e+00,  1.3066e+00, -7.5367e-01],\n",
      "          [-1.1262e+00, -1.0974e-01, -9.9919e-01, -3.6324e-01,  1.3452e+00,\n",
      "           -2.1681e+00, -6.0380e-01, -5.1099e-01, -7.9566e-03, -1.4305e-02,\n",
      "            1.3624e+00,  4.8274e-01, -3.6238e-03, -4.3688e-01, -9.7639e-01,\n",
      "            6.8688e-01,  1.6063e+00,  9.2064e-01,  2.7532e-01,  1.2058e+00,\n",
      "           -7.1562e-01,  2.8207e-01,  1.8068e+00, -1.5365e+00,  7.6825e-01,\n",
      "           -1.4553e-01,  8.5631e-01,  4.4034e-01],\n",
      "          [ 1.1073e+00, -1.3627e+00, -8.7349e-01, -2.6496e-01, -2.9847e-01,\n",
      "            1.0852e-02,  7.6670e-01,  1.7074e-01, -6.4518e-01, -2.8532e-01,\n",
      "           -1.3669e+00,  5.7853e-01, -8.1534e-02,  2.3105e+00,  1.1758e+00,\n",
      "           -5.4959e-01, -3.2913e-01,  7.8588e-01, -1.9530e+00,  3.2521e-01,\n",
      "           -1.9237e+00,  1.1718e+00, -1.8582e+00, -1.1760e+00,  1.0016e+00,\n",
      "           -4.7900e-01,  1.1980e+00, -1.2322e+00],\n",
      "          [ 5.8917e-01,  7.5846e-01, -5.1932e-01,  3.7347e-01, -3.8033e-02,\n",
      "           -1.3701e+00, -1.2279e+00,  1.3721e-01,  4.8313e-01,  1.3050e+00,\n",
      "            7.8368e-01,  1.4157e+00, -6.7210e-01,  1.9867e-02,  9.7441e-01,\n",
      "           -1.1492e+00,  3.4417e-01, -1.8073e+00,  1.5996e+00,  1.7721e+00,\n",
      "            2.9123e-02, -1.5352e-01,  1.4039e+00, -9.1839e-01,  9.9926e-01,\n",
      "           -9.6252e-01,  1.3858e-01, -3.8797e-01],\n",
      "          [ 2.2421e-01,  1.2953e+00,  2.8066e-01,  1.7495e+00, -1.0202e+00,\n",
      "           -3.0031e-01,  5.4101e-01,  1.2746e+00,  1.0471e+00, -7.0015e-01,\n",
      "           -4.7718e-01, -1.6333e-01,  1.2370e+00, -2.1927e-01, -9.3897e-01,\n",
      "            4.7807e-01,  1.3434e-02, -6.1660e-01, -1.1087e+00,  1.4852e+00,\n",
      "           -1.9971e+00,  1.9532e+00,  1.5381e-01, -2.8959e-01, -1.7340e-01,\n",
      "           -3.1851e-01,  2.3394e-01,  2.4997e-01]]]])]\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "# ort_sessions = onnxruntime .... et le reste je l'ai pas\n",
    "# https://pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html#compare-the-pytorch-results-with-the-ones-from-the-onnx-runtime\n",
    "\n",
    "onnx_input = [torch_input]\n",
    "print(f\"Input length: {len(onnx_input)}\")\n",
    "print(f\"Sample input: {onnx_input}\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./model.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
    "\n",
    "# onnxruntime returns a list of outputs\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
